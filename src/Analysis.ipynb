{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of COVID impact on US household"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Motivation </h3> \n",
    "\n",
    "<p>The goal of this analysis is to gauge the impact of the pandemic on overall household characteristics such as employment status, housing, education disruptions, and dimensions of physical and mental wellness. There is a large amount of emotionally negative stimuli related to the COVID-19 pandemic. How do people prepare themselves in difficult times like this? Analyzing and exploring people's response to pandemic can provide useful insights into people's perspective about COVID and the challenges they face.</p>\n",
    "\n",
    "<p>As we all know,the impacts of the pandemic and the economic fallout have been widespread, but are particularly prevalent among Black, Latino, Indigenous, and immigrant households. There is also an impact on gender. This analysis will deep dive into some of the impacts of COVID by Age group, race and ethinicity, and gender. We also try to understand the different groups of people based on various characteristics pertaining to COVID. The research questions will target specific variables. Below are some references that tracks the COVID impacts: </p>\n",
    "<li><a href='https://www.cbpp.org/research/poverty-and-inequality/tracking-the-covid-19-recessions-effects-on-food-housing-and'>Covid Recession effects</a></li>\n",
    "<li><a href='https://www.cdc.gov/nchs/covid19/pulse/mental-health.htm'>Covid data from NCHS</a></li>\n",
    "\n",
    "\n",
    "<h3>Data Source</h3>\n",
    "<p>The <a href='https://www2.census.gov/programs-surveys/demo/technical-documentation/hhp/2020_HPS_Background.pdf'>Household Pulse Survey</a> provides timely data to help understand the experiences of American households during the coronavirus pandemic. Data for this analysis is obtained from the Phase 1 Household Pulse Survey that began on April 23 and ended on July 21, 2020. The dataset is very rich and informative. It dataset has 105 variables, 1088314 observations and includes employment status, food security, housing, physical and mental health, access to health care, and educational disruption. In order to support the nation’s recovery, we need to know the ways this pandemic has affected people’s lives and livelihoods. Data from these datasets will show the widespread effects of the coronavirus pandemic on individuals, families, and communities across the country. </p>\n",
    "\n",
    "<p>The survey was conducted by an internet questionnaire, with invitations to participate sent by email and text message. Housing units linked to one or more email addresses or cell phone numbers were randomly selected to participate, and one respondent from each housing unit was selected to respond. All the data has been de-identified.</p>\n",
    "\n",
    "<h4> Links to Data set and Data dictionary</h4>\n",
    "<ul><li>The Phase 1 survey datasets are available for public use under <a href='https://www.census.gov/programs-surveys/household-pulse-survey/datasets.html'>census.gov</a> website as weekly files.</li>\n",
    "<li>Data dictionary is available in the census.gov website under the link <a href='https://www.census.gov/programs-surveys/household-pulse-survey/technical-documentation.html#phase1'>Phase 1 Household Pulse Survey Technical Documentation</a></li></ul>\n",
    "\n",
    "<h4>Download data</h4>\n",
    "<p>Data is directly downloaded from census website using zipfile package<p>\n",
    "  \n",
    "\n",
    "<h4>Terms of use of census data </h4>\n",
    "<p>The Census Bureau is committed to open government by sharing its public data as open data. Census data continues to be a key national resource, serving as a fuel for entrepreneurship and innovation, scientific discovery, and commercial activity.  We continuously identify and publish datasets and Application Programming Interface’s (API’s) to Data.gov in accordance with the Office of Management and Budget (OMB) Memorandum M-10-06, the Executive Order 13642 on open data, and the overall principles outlined in the Digital Government Strategy.  In\n",
    " accordance with the Open Data Policy, M-13-13, the Census Bureau publishes its information in machine-readable formats while also safeguarding privacy and security.</p>\n",
    " \n",
    " \n",
    " <h3>Research Questions</h3>\n",
    "<ul>\n",
    "    <li>Understand the impacts of COVID in terms of employment loss, income loss, food insufficiency, education interruptions, inability to meet housing expenses and how does this vary by Race/Ethnicity or gender? </li>\n",
    "    <li>What is the impact on Mental health status (Anxiety and depression)? Is there a correlation between Mental health status (Anxiety and depression) and factors such as age, number of household members, gender, income, health status, race? How does the anxiety levels vary between first and last week of survey?</li>\n",
    "    <li>How does employment loss, income loss, food insufficiency, education interruptions, inability to meet housing expenses in Washington differ as compared  to national average?</li>\n",
    "    <li>How do different groups based on age, race and ethnicity differ in their behavior or attitude towards COVID. Are there any patterns observed in the population based on certain characteristics pertaining to COVID?</li> \n",
    "</ul>\n",
    "\n",
    "\n",
    "<h3>Methodology</h3>\n",
    "<p>For all the research questions, multivariate analysis will be used.</p>\n",
    "<p><strong>Statistical Analysis Method</strong>\n",
    "<ul><li>Regression analysis will be used to train and determine the impactful predictors. This method is appropriate as all the data points are independent and the sample size is large enough to meet the normality assumption. As the dataset has both numerical and categorical data, regression techniques are suitable. The model is also interpretable.</li>\n",
    "<li>Clustering will be used to identify any patterns and classify groups of people based on similar characteristics</li></ul>\n",
    "<li><strong>Results</strong>\n",
    "    The results will be presented in a comprehensive compilation of visualizations.</li></p>\n",
    "\n",
    "<h3>Source of Bias</h3>\n",
    "<p> Nonsampling errors can also occur and are more likely for surveys that are implemented quickly, achieve low response rates, and rely on online response.  Nonsampling errors for the Household Pulse Survey may include:</p>\n",
    "\n",
    "<ul><li><strong>Measurement error:</strong> The respondent provides incorrect information, or an unclear survey question is misunderstood by the respondent. The Household Pulse Survey schedule offered only limited time for testing questions. </li>\n",
    "<li><strong>Coverage error: </strong>Individuals who otherwise would have been included in the survey frame were missed. The Household Pulse Survey only recruited households for which an email address or cell phone number could be identified.</li>\n",
    "<li><strong>Nonresponse error:</strong> Responses are not collected from all those in the sample or the respondent is unwilling to provide information. The response rate for the Household Pulse Survey was substantially lower than most federally sponsored surveys.</li>\n",
    "<li><strong>Processing error: </strong>Forms may be lost, data may be incorrectly keyed, coded, or recoded. The real-time dissemination of the Household Pulse Survey provided limited time to identify and fix processing errors.</li>\n",
    " \n",
    "<p>The Census Bureau employs quality control procedures to minimize these errors.  However, the potential bias due to nonsampling errors has not yet been evaluated.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Exploratory Data Analysis </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Import Packages</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# packages for preprocessing and standardization\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, normalize\n",
    "\n",
    "# packages required for k-means and PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# packages for statistical analysis\n",
    "from scipy import stats\n",
    "from scipy.stats import chi2_contingency\n",
    "from scipy.stats import chi2\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.formula.api import logit\n",
    "\n",
    "# Packages for interactive widgets and trees\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import export_graphviz # display the tree within a Jupyter notebook\n",
    "from IPython.display import SVG\n",
    "from graphviz import Source\n",
    "from IPython.display import display\n",
    "from ipywidgets import interactive, IntSlider, FloatSlider, interact\n",
    "import ipywidgets\n",
    "from IPython.display import Image\n",
    "from subprocess import call\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "# packages for featre importance analysis\n",
    "from yellowbrick.model_selection import FeatureImportances\n",
    "\n",
    "# skelearn packages for regression\n",
    "from sklearn import utils\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1. Understand the impacts of COVID in terms of employment loss, income loss, food insufficiency, education interruptions, inability to meet housing expenses and how does this vary by Race/Ethnicity or gender? Are minority groups and women affected the most?</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Read the clean data csv file into a pandas dataframe df1</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('C:/Users/Lakshmi/Desktop/UW DataScience application/Data 512/covid_clean_data.csv', sep=',', header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Create a dataframe df1_q1 with the variables of interest 'RACE_ETHNICITY','EGENDER','EMPLOSSCOVID','FOOD_INSUFF','RENT_DEBT','EDUC_DISRUPT','INCOMELOSS' from the dataframe df1 using copy(). Copy() ensures any changes in df1_q1 doesn't affect the original datframe df1</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_q1 = df1[['RACE_ETHNICITY','EGENDER','EMPLOSSCOVID','FOOD_INSUFF','RENT_DEBT','EDUC_INTERUPT','INCOMELOSS']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_q1_pivot = pd.melt(df1_q1,id_vars=['RACE_ETHNICITY','EGENDER'], var_name=['INDICATOR'], value_name='VALUE')\n",
    "df1_q1_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.RACE_ETHNICITY.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(index=[df1_q1_pivot['INDICATOR'],df1_q1_pivot['RACE_ETHNICITY']], columns=df1_q1_pivot['VALUE'],normalize='index',margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30, 30))\n",
    "\n",
    "sns.set_context(\"notebook\", font_scale=3, rc={\"lines.linewidth\":2})\n",
    "\n",
    "graph = sns.barplot(x=\"VALUE\", \n",
    "              y=\"INDICATOR\",\n",
    "              edgecolor='black',\n",
    "              linewidth=.5,\n",
    "              hue = 'RACE_ETHNICITY',\n",
    "              data=df1_q1_pivot,\n",
    "              palette=\"pastel\")\n",
    "              #order=df1_q1_pivot.sort_values('VALUE').INDICATOR)\n",
    "\n",
    "\n",
    "for i in graph.patches:\n",
    "    # get_width pulls left or right; get_y pushes up or down\n",
    "    graph.text(i.get_width()-.045, i.get_y()+.11, \\\n",
    "            str(round((i.get_width())*100, 2))+'%', fontsize=30, color='black') \n",
    "\n",
    "\n",
    "graph.set(xlabel='% of Total', title='Employment loss, income loss, education disruptions, food insufficiency, rent debt by Race/Ethnicity')\n",
    "\n",
    "\n",
    "L  = plt.legend(bbox_to_anchor=(1.05, 1), loc=1, borderaxespad=0.)\n",
    "\n",
    "graph.savefig('C:/Users/Lakshmi/Desktop/UW DataScience application/Data 512/Final project pictures/CovidImpactByRaceEthnicity.jpg');\n",
    "plt.show(graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.EGENDER.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(index=[df1_q1_pivot['INDICATOR'],df1_q1_pivot['EGENDER']], columns=df1_q1_pivot['VALUE'],normalize='index',margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30, 30))\n",
    "sns.set_context(\"notebook\", font_scale=5, rc={\"lines.linewidth\":2})\n",
    "\n",
    "graph = sns.barplot(x=\"VALUE\", \n",
    "              y=\"INDICATOR\",\n",
    "              edgecolor='black',\n",
    "              linewidth=.5,\n",
    "              hue = 'EGENDER',\n",
    "              data=df1_q1_pivot,\n",
    "              palette=\"pastel\")\n",
    "              #order=df1_q1_pivot.sort_values('VALUE').INDICATOR)\n",
    "\n",
    "\n",
    "for i in graph.patches:\n",
    "    # get_width pulls left or right; get_y pushes up or down\n",
    "    graph.text(i.get_width()-.04, i.get_y()+.22, \\\n",
    "            str(round((i.get_width())*100, 2))+'%', fontsize=35, color='black') \n",
    "\n",
    "\n",
    "graph.set(xlabel='% of Total', title='Employment loss, income loss, food insufficiency, rent debt, education disruptions by gender')\n",
    "\n",
    "\n",
    "L  = plt.legend(bbox_to_anchor=(1.05, 1), loc=1, borderaxespad=0.)\n",
    "\n",
    "graph.savefig('C:/Users/Lakshmi/Desktop/UW DataScience application/Data 512/Final project pictures/CovidImpactByGender.jpg')\n",
    "plt.show(graph)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Bias : It can be seen that white people are over represented than other races.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Hypothesis test</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars = ['EMPLOSSCOVID','FOOD_INSUFF','RENT_DEBT','EDUC_INTERUPT','INCOMELOSS']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul><strong>Null Hypotheses H0: </strong> \n",
    "    <ul><li>There is no difference in the average Employment loss, food insufficiency, rent debt, education interruption, income loss by Race/Ethnicity groups</li>\n",
    "    <li>There is no difference in the average Employment loss, food insufficiency, rent debt, education interruption, income loss by gender type.</li></ul>\n",
    "    \n",
    " <strong>Alternate Hypotheses H1: </strong>\n",
    "    <ul><li>There is difference in the average Employment loss, food insufficiency, rent debt, education interruption, income loss by Race/Ethnicity groups</li>\n",
    "    <li>There is difference in the average Employment loss, food insufficiency, rent debt, education interruption, income loss by gender type.</li></ul></ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cannot use ANOVA as dependent variables are binary and we need Normally-distributed dependent variable. Other assumptions of ANOVA are Homogeneity of variance (a.k.a. homoscedasticity) and  Independence of observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The  chi's square test of independence is tests for dependence between categorical variables and is an omnibus test.\n",
    "Checking the assumptions for the  test of independence is easy. Let's recall what they are:</p>\n",
    "<li>The two samples are independent</li>\n",
    "<li>The variables were collected independently of each other, i.e. the answer from one variable was not dependent on the answer of the other</li>\n",
    "<li>No expected cell count is = 0</li>\n",
    "<li>No more than 20% of the cells have and expected cell count < 5</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for var in vars:\n",
    "    table = pd.crosstab(df1[var],df1.EGENDER)\n",
    "    stat, p, dof, expected = chi2_contingency(table)\n",
    "    \n",
    "    prob = 0.95\n",
    "    critical = chi2.ppf(prob, dof)\n",
    "    #print('probability=%.3f, critical=%.3f, stat=%.3f, dof=%.3f' % (prob, critical, stat, dof))\n",
    "    #dof = (rows - 1) * (cols - 1)\n",
    "    if abs(stat) >= critical:\n",
    "        print(var, 'is associated with gender: Dependent (reject H0)')\n",
    "    else:\n",
    "        print(var, 'is not associated with gender: Independent (fail to reject H0)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for var in vars:\n",
    "    table = pd.crosstab(df1[var],df1.RACE_ETHNICITY)\n",
    "    stat, p, dof, expected = chi2_contingency(table)\n",
    "    \n",
    "    prob = 0.95\n",
    "    critical = chi2.ppf(prob, dof)\n",
    "    #print('probability=%.3f, critical=%.3f, stat=%.3f, dof=%.3f' % (prob, critical, stat, dof))\n",
    "    #dof = (rows - 1) * (cols - 1)\n",
    "    if abs(stat) >= critical:\n",
    "        print(var, 'is associated with Race/Ethnicity: Dependent (reject H0)')\n",
    "    else:\n",
    "        print(var, 'is not associated with Race/Ethnicity: Independent (fail to reject H0)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>As response variables are binomial in nature, we can use logistic regression analysis here</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in vars:\n",
    "    print(df1[var].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimates_interpret(df,var,model):\n",
    "\n",
    "    for i in range(1,len(df.coef)+1):\n",
    "        if(df.coef[i] > 0):\n",
    "            if(model == \"binom\"):                \n",
    "                print(\"The odds of\",var,\"are higher by about\", round(np.exp(df.coef[i])*100 - 100,2) \\\n",
    "                      , \"% for\",df.predictors[i], \"compared to the reference group keeping other predictors constant\")\n",
    "            else:\n",
    "                print(\"The risk of\",var,\"increases by about\", round(np.exp(df.coef[i])*100 - 100,2) \\\n",
    "                      , \"% for\",df.predictors[i], \"compared to the reference group keeping other predictors constant\")\n",
    "        else: \n",
    "            if(model == \"binom\"):  \n",
    "                print(\"The odds of\",var,\"are lower by about\", round(100 - np.exp(df.coef[i])*100,2) \\\n",
    "                      , \"% for\",df.predictors[i], \"compared to the reference group keeping other predictors constant\")\n",
    "            else:\n",
    "                print(\"The risk of\",var,\"decreases by about\", round(100 - np.exp(df.coef[i])*100,2) \\\n",
    "                      , \"% for\",df.predictors[i], \"compared to the reference group keeping other predictors constant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>The default SEs from glm are correct for logistic regression with a binary\n",
    "response, as long as the sample size is sufficiently large</strong>\n",
    "<li>We always need a large sample size which is the case here</li>\n",
    "<li>We never assume normality of errors</li>\n",
    "<li>We use N(0,1) distribution to calculate the p-value</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from patsy import dmatrices\n",
    "scaler = StandardScaler()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in vars:\n",
    "    y, X = dmatrices(var + \" ~ C(EGENDER, Treatment(reference='FEMALE')) + C(RACE_ETHNICITY, Treatment(reference='White alone'))\", df1, return_type = 'dataframe')\n",
    "    X_std = scaler.fit_transform(X)\n",
    "    model = LogisticRegression(C = 1e9)\n",
    "    res = model.fit(X_std, y.values.ravel())\n",
    "    y_pred = model.predict_proba(X_std)[:, 1]\n",
    "    \n",
    "    #model.coef_\n",
    "    print()\n",
    "    print(\"**************Response variable = \",var, \"************** : \")\n",
    "    df = pd.concat([pd.DataFrame(X.columns),pd.DataFrame(np.transpose(model.coef_))], axis = 1)\n",
    "    df.drop(df.index[[0]],axis=0,inplace=True)\n",
    "    df.columns = pd.io.parsers.ParserBase({'names':df.columns})._maybe_dedup_names(df.columns) \n",
    "    df.rename(columns = {df.columns[0]:'predictors',df.columns[1]:'coef'}, inplace = True)\n",
    "    print(df)\n",
    "    print(\"******************************************************* : \")\n",
    "    print(\"Pseudo R^2 scores: \",r2_score(y, y_pred))\n",
    "    print()\n",
    "    estimates_interpret(df,var,\"binom\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Likelihood Ratio Test: Testing the Joint Significance of All Predictors.</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deviance Test for logisitic regression\n",
    "#Does the model that includes the variable(s) in question tell us more about \n",
    "#the outcome (or response) variable than a model that does not include the variable(s)?\n",
    "stats.chisqprob = lambda chisq, df: stats.chi2.sf(chisq, df)\n",
    "    \n",
    "for var in vars:\n",
    "    print()\n",
    "    print(\"**************Response variable = \",var, \"************** : \")\n",
    "    data = df1[[var,'EGENDER','RACE_ETHNICITY']].copy()\n",
    "    glm_binom_full = smf.glm(var + ' ~ 1 + C(RACE_ETHNICITY) + C(EGENDER)', data=data, family=sm.families.Binomial())\n",
    "    glm_binom_red = smf.glm(var + ' ~ 1', data=data, family=sm.families.Binomial())\n",
    "    res_full = glm_binom_full.fit()\n",
    "    res_red = glm_binom_red.fit()\n",
    "    D1 = res_full.deviance\n",
    "    D0 = res_red.deviance\n",
    "    LR = D0 - D1\n",
    "    print(\"Likelihood ratio: \",LR)\n",
    "    print(\"Pseudo R^2: \",1 - (res_full.llf/res_red.llf))\n",
    "    print(\"Log-Likelihood-full: \",res_full.llf)\n",
    "    print(\"Log-Likelihood-red: \",res_red.llf)\n",
    "    df=res_full.df_model - res_red.df_model\n",
    "    p_value = stats.chisqprob(LR, df)\n",
    "    print(\"Overall Likelihood Ratio Test p-value: \",p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>We can infer from these results that: </h4>\n",
    " <ul><li>As overall LRT p-value is significant, we prefer the full model over the null model.</li>\n",
    "    <li>As pseudo R^2 is close to 0, the model fit is not great and can be improved by adding additional predictors besides gender adn race/ethnicity, but here we are exploring only gender and race/ethnicity</li>\n",
    "    <li>The individual p-values and the overall LRT hypothesis p-values are significant and we reject null Hypothesis and conclude that</li>\n",
    "    <ul><li>There is difference in the average Employment loss, food insufficiency, rent debt, education interruption, income loss by Race/Ethnicity groups</li>\n",
    "    <li>There is difference in the average Employment loss, food insufficiency, rent debt, education interruption, income loss by gender type.</li></ul>\n",
    "     <li>The average Employment loss, food insufficiency, rent debt, education interruption, income loss are higher for females and minority ethnic groups</li></ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2. What is the impact on Mental health status (Anxiety and depression)? Is there a correlation between Mental health status (Anxiety and depression) and factors such as age, number of household members, gender, income, health status, race? How does the anxiety levels vary between first and last week of survey?</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df1.ANXIETY_DEPRESSION.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul><strong>We can infer from the above graph that</strong>\n",
    "    <li>For each age group and race/ethnic groups, Anxiety_depression is high for income level less than 25000 dollars.  Anxiety and depression indicator seems to be negatively correlated with income level. Higher the income, lower is the Anxiety_Depression</li>\n",
    "    <li>Anxiety_depression is relatively lower for age_group 65 and above as compared to other age groups</li>\n",
    "    <li>Anxiety_depression is fluctuating in the age group 18-24 for all but white</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_health_anxiety = df1[['HLTHSTATUS','ANXIETY_DEPRESSION','WEEK','INTEREST','WORRY',\\\n",
    "                          'AGE_GROUP','INCOME_LEV','RACE_ETHNICITY','EGENDER','THHLD_NUMPER']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_hlth_anx_pivot = pd.melt(df1_health_anxiety,id_vars=['WEEK','AGE_GROUP','INCOME_LEV','RACE_ETHNICITY','EGENDER','THHLD_NUMPER'], var_name=['INDICATOR'], value_name='VALUE')\n",
    "df1_hlth_anx_pvt = df1_hlth_anx_pivot.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_hlth_anx_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_mental = pd.crosstab(index=[df1_hlth_anx_pivot['WEEK'],df1_hlth_anx_pivot['INDICATOR']], columns=df1_hlth_anx_pivot['VALUE'],normalize='index',margins=True)\n",
    "df1_mental.columns.rename(None, inplace=True)\n",
    "df1_mental.reset_index(inplace=True)\n",
    "#df1_mental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_mental = pd.melt(df1_mental,id_vars=['WEEK','INDICATOR'], var_name='Indicator Value', value_name='VALUE')\n",
    "df1_mental= df1_mental[~df1_mental.INDICATOR.isin([''])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_mental['INDICATOR']= df1_mental['INDICATOR'] + ' - '  + df1_mental['Indicator Value'] \n",
    "df1_mental.drop('Indicator Value',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_mental = df1_mental[(df1_mental.VALUE != 0)]\n",
    "df1_mental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(40, 25))\n",
    "sns.set_context(\"notebook\", font_scale=5, rc={\"lines.linewidth\":2})\n",
    "\n",
    "#graph = sns.barplot(x=\"WEEK\", y=\"per\", hue='INDICATOR', edgecolor='black',linewidth=.5,data=df1_hlth_anx_pivot)\n",
    "graph = sns.lineplot(x=\"WEEK\", y=\"VALUE\", hue='INDICATOR', legend=\"full\", marker=True,linewidth=9,data=df1_mental)\n",
    "\n",
    "graph.set(xlabel='Week', ylabel=\"% of Total participants\", title='Mental Health Indicators')\n",
    "\n",
    "\n",
    "leg = plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.,fontsize=30)\n",
    "\n",
    "\n",
    "for legobj in leg.legendHandles:\n",
    "    legobj.set_linewidth(9.0)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "graph.savefig('C:/Users/Lakshmi/Desktop/UW DataScience application/Data 512/Final project pictures/AllMentalIndicators.jpg')\n",
    "plt.show(graph);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_mental= df1_mental[~df1_mental.INDICATOR.isin(['WORRY - MODERATE','INTEREST - MODERATE','ANXIETY_DEPRESSION - MODERATE','ANXIETY_DEPRESSION - NONE','INTEREST - VERY HIGH','HLTHSTATUS - EXCELLENT','HLTHSTATUS - FAIR','WORRY - NONE'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(40, 25))\n",
    "sns.set_context(\"notebook\", font_scale=5, rc={\"lines.linewidth\":2})\n",
    "\n",
    "#graph = sns.barplot(x=\"WEEK\", y=\"per\", hue='INDICATOR', edgecolor='black',linewidth=.5,data=df1_hlth_anx_pivot)\n",
    "graph = sns.lineplot(x=\"WEEK\", y=\"VALUE\", hue='INDICATOR', legend=\"full\", marker=True,linewidth=9,data=df1_mental)\n",
    "\n",
    "graph.set(xlabel='Week', ylabel=\"% of Total participants\", title='Mental Health Indicators')\n",
    "\n",
    "\n",
    "leg = plt.legend(bbox_to_anchor=(1.05, 1), loc=4, borderaxespad=0.,fontsize=45)\n",
    "\n",
    "\n",
    "for legobj in leg.legendHandles:\n",
    "    legobj.set_linewidth(9.0)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "graph.savefig('C:/Users/Lakshmi/Desktop/UW DataScience application/Data 512/Final project pictures/SelectedMentalIndicators.jpg')\n",
    "plt.show(graph);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>We can infer from the above graph that Week  after week, Avg health status is declining and Anxiety_depression, interest and worry indicators increase</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_mental_age_inc = pd.crosstab(index=[df1_hlth_anx_pvt['WEEK'],df1_hlth_anx_pvt['INDICATOR'],\\\n",
    "                                df1_hlth_anx_pvt['AGE_GROUP'],df1_hlth_anx_pvt['INCOME_LEV']],\\\n",
    "                             columns=df1_hlth_anx_pvt['VALUE'],normalize='index',margins=True)\n",
    "df1_mental_age_inc.columns.rename(None, inplace=True)\n",
    "df1_mental_age_inc.reset_index(inplace=True)\n",
    "df1_mental_age_inc = df1_mental_age_inc[df1_mental_age_inc.WEEK != 'All']\n",
    "\n",
    "df1_mental_age_inc = pd.melt(df1_mental_age_inc,id_vars=['WEEK','INDICATOR','AGE_GROUP','INCOME_LEV'], \\\n",
    "                             var_name='Indicator Value', value_name='VALUE')\n",
    "df1_mental_age_inc = df1_mental_age_inc.loc[(df1_mental_age_inc['INDICATOR'] =='ANXIETY_DEPRESSION'),:]\n",
    "\n",
    "df1_mental_age_inc['INDICATOR']= df1_mental_age_inc['INDICATOR'] + ' - '  + df1_mental_age_inc['Indicator Value'] \n",
    "df1_mental_age_inc.drop('Indicator Value',axis=1,inplace=True)\n",
    "\n",
    "df1_mental_age_inc = df1_mental_age_inc[(df1_mental_age_inc.VALUE != 0)]\n",
    "df1_mental_age_inc.reset_index(drop=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context(\"poster\", font_scale=1.7, rc={\"lines.linewidth\":6})\n",
    "\n",
    "\n",
    "graph = sns.FacetGrid(df1_mental_age_inc,hue='INDICATOR', legend_out = False,\n",
    "                      col='INCOME_LEV', row='AGE_GROUP', height=8, aspect=1.2, \\\n",
    "                      margin_titles=True,palette='muted')\n",
    "graph = (graph.map(sns.lineplot, 'WEEK', 'VALUE',ci=None))\n",
    "[plt.setp(ax.texts, text=\"\") for ax in graph.axes.flat] \n",
    "for ax in graph.axes.flat:\n",
    "    box = ax.get_position()\n",
    "    ax.set_position([box.x0,box.y0,box.width*0.9,box.height])\n",
    "graph.set_titles(row_template = '{row_name}', col_template = '{col_name}')\n",
    "graph = graph.set_axis_labels(\"Week\", \"% of total\")\n",
    "\n",
    "graph.set_xticklabels(\n",
    "    rotation=90,\n",
    "    horizontalalignment='right',\n",
    ")\n",
    "\n",
    "plt.tick_params(labelsize=30)\n",
    "\n",
    "leg = plt.legend(bbox_to_anchor=(1.05, 1), loc=4, borderaxespad=0.,fontsize=30)\n",
    "\n",
    "\n",
    "for legobj in leg.legendHandles:\n",
    "    legobj.set_linewidth(15.0)\n",
    "    \n",
    "#plt.subplots_adjust(top=0.8)\n",
    "\n",
    "plt.suptitle(\"Anxiety Depression by Age_group and Income level\", fontsize=55, y=1.02)\n",
    "\n",
    "plt.tight_layout()\n",
    "graph.savefig('C:/Users/Lakshmi/Desktop/UW DataScience application/Data 512/Final project pictures/AnxietyByAgeAndIncome.jpg')\n",
    "plt.show(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We can infer from the above graph that: </p>\n",
    "<ul><li>For all age groups, with increase in income earnings, moderate and very high Anxiety/depression percentage decrease and no anxiety/depression percentage increase</li>\n",
    "<li>For each income level, as age increases, moderate and very high Anxiety/depression percentage decrease and no anxiety/depression percentage increase</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_mental_all = pd.crosstab(index=[df1_hlth_anx_pvt['WEEK'],df1_hlth_anx_pvt['INDICATOR'],\\\n",
    "                                    df1_hlth_anx_pvt['EGENDER'], \\\n",
    "                                df1_hlth_anx_pvt['RACE_ETHNICITY']],\\\n",
    "                             columns=df1_hlth_anx_pvt['VALUE'],normalize='index',margins=True)\n",
    "df1_mental_all.columns.rename(None, inplace=True)\n",
    "df1_mental_all.reset_index(inplace=True)\n",
    "df1_mental_all = df1_mental_all[df1_mental_all.WEEK != 'All']\n",
    "\n",
    "\n",
    "df1_mental_all = pd.melt(df1_mental_all,id_vars=['WEEK','INDICATOR','RACE_ETHNICITY',\\\n",
    "                         'EGENDER'], var_name='Indicator Value', value_name='VALUE')\n",
    "df1_mental_all = df1_mental_all.loc[(df1_mental_all['INDICATOR'] =='ANXIETY_DEPRESSION'),:]\n",
    "\n",
    "df1_mental_all['INDICATOR']= df1_mental_all['INDICATOR'] + ' - '  + df1_mental_all['Indicator Value'] \n",
    "df1_mental_all.drop('Indicator Value',axis=1,inplace=True)\n",
    "#df1_mental_all = df1_mental_all[df1_mental_all['INDICATOR'] != 'ANXIETY_DEPRESSION - NONE']\n",
    "#df1_mental_all= df1_mental_all[~df1_mental_all.INDICATOR.isin(['ANXIETY_DEPRESSION - NONE'])]\n",
    "\n",
    "df1_mental_all = df1_mental_all[(df1_mental_all.VALUE != 0)]\n",
    "df1_mental_all.reset_index(drop=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context(\"poster\", font_scale=1.7, rc={\"lines.linewidth\":6})\n",
    "\n",
    "graph = sns.FacetGrid(df1_mental_all,hue='INDICATOR', legend_out = False,\n",
    "                      row='EGENDER', col='RACE_ETHNICITY', height=8, aspect=1.2, \\\n",
    "                      margin_titles=True,palette='muted')\n",
    "graph = (graph.map(sns.lineplot, 'WEEK', 'VALUE',ci=None))\n",
    "[plt.setp(ax.texts, text=\"\") for ax in graph.axes.flat] \n",
    "for ax in graph.axes.flat:\n",
    "    box = ax.get_position()\n",
    "    ax.set_position([box.x0,box.y0,box.width*1,box.height])\n",
    "graph.set_titles(row_template = '{row_name}', col_template = '{col_name}')\n",
    "graph = graph.set_axis_labels(\"Week\", \"% of total\")\n",
    "\n",
    "graph.set_xticklabels(\n",
    "    rotation=90, \n",
    "    horizontalalignment='right',\n",
    ")\n",
    "\n",
    "plt.tick_params(labelsize=30)\n",
    "\n",
    "leg = plt.legend(bbox_to_anchor=(1.05, 1), loc=4, borderaxespad=0.,fontsize=35)\n",
    "\n",
    "for legobj in leg.legendHandles:\n",
    "    legobj.set_linewidth(15.0)\n",
    "    \n",
    "plt.suptitle(\"Anxiety Depression by Race/Ethnicity and Gender\", fontsize=55, y=1.02)\n",
    "    \n",
    "plt.tight_layout()\n",
    "graph.savefig('C:/Users/Lakshmi/Desktop/UW DataScience application/Data 512/Final project pictures/AnxietyByRaceAndGender.jpg')\n",
    "plt.show(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We can infer from the above graph that: </p>\n",
    "<ul><li>For all age groups, with increase in income earnings, moderate and very high Anxiety/depression percentage decrease and no anxiety/depression percentage increase</li>\n",
    "<li>For each income level, as age increases, moderate and very high Anxiety/depression percentage decrease and no anxiety/depression percentage increase</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.WORRY.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars=['EGENDER','WORRY','INTEREST','INCOMELOSS','FOOD_INSUFF',\\\n",
    "      'AGE_GROUP','THHLD_NUMPER','INCOME_LEV','HLTHSTATUS','RACE_ETHNICITY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for var in vars:\n",
    "    table = pd.crosstab(df1.ANXIETY_DEPRESSION,df1[var])\n",
    "    stat, p, dof, expected = chi2_contingency(table)\n",
    "    \n",
    "    prob = 0.95\n",
    "    critical = chi2.ppf(prob, dof)\n",
    "    #print('probability=%.3f, critical=%.3f, stat=%.3f, dof=%.3f' % (prob, critical, stat, dof))\n",
    "    #dof = (rows - 1) * (cols - 1)\n",
    "    if abs(stat) >= critical:\n",
    "        print('ANXIETY_DEPRESSION is associated with '  + var + ': Dependent (reject H0)')\n",
    "    else:\n",
    "        print('ANXIETY_DEPRESSION is not associated with ' + var + ': Independent (fail to reject H0)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_q2 = df1[vars].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_q2['ANXIETY_DEPRESSION'] = df1['ANXIETY_DEPRESSION'].copy()\n",
    "df1_q2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason for doing the analysis with Ordinal Logistic Regression is that the dependent variable is categorical and ordered. The dependent variable of the dataset is 'ANXIETY_DEPRESSION', which has three ranked levels — 'NONE','MODERATE','VERY HIGH'. Ordinal Logistic Regression takes account of this order and return the contribution information of each independent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convert the pandas dataframe df1_q2 into R dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import rpy2.robjects as ro\n",
    "from rpy2.robjects import pandas2ri\n",
    "from rpy2.robjects.packages import importr\n",
    "from rpy2.robjects.conversion import localconverter\n",
    "\n",
    "with localconverter(ro.default_converter + pandas2ri.converter):\n",
    "    R_df = ro.conversion.py2rpy(df1_q2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we do ordered logistic regression using MASS package from R and get the summary into coeff variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mass = importr(\"MASS\")\n",
    "R = ro.r\n",
    "\n",
    "base = importr('base')\n",
    "stats = importr('stats')\n",
    "\n",
    "model = mass.polr('as.factor(ANXIETY_DEPRESSION) ~ EGENDER + WORRY + INTEREST + \\\n",
    "        as.factor(INCOMELOSS) + as.factor(FOOD_INSUFF) + AGE_GROUP + \\\n",
    "        as.factor(THHLD_NUMPER) + INCOME_LEV + HLTHSTATUS \\\n",
    "        + RACE_ETHNICITY',R_df, Hess = True, method=\"logistic\")\n",
    "\n",
    "coeff = base.summary(model).rx2('coefficients')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convert the results from FloatMatrix in R to 2D array in Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with localconverter(ro.default_converter + pandas2ri.converter):\n",
    "    coeff_pd = ro.conversion.rpy2py(coeff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct pandas dataframe from 2D array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(coeff_pd, columns =['coeff', 'Std. Error','t value']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['names'] = coeff.names[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reorder columns in pandas dataframe df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['names','coeff', 'Std. Error','t value']\n",
    "df = df[cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate p-value from t-statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "n = len(df1)\n",
    "df['p_value'] = stats.t.sf(np.abs(df['t value']),n-1)*2\n",
    "df['Odds Ratio'] = np.exp(df['coeff'])\n",
    "df['significance'] = (df['p_value'] <= 0.05)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_ordcoef(df):\n",
    "    for i in range(0,len(df.coeff)-2):\n",
    "        if((df.significance[i] == True) & (df.coeff[i] > 0)):\n",
    "            print(\"For participants having\", df.names[i],\", the odds of having moderate or very high Anxiety/Depression is\", round(np.exp(df.coeff[i])*100 - 100,2) \\\n",
    "                          , \"% higher compared to the reference group, keeping other predictors constant\")\n",
    "            print()\n",
    "        elif((df.significance[i] == True) & (df.coeff[i] < 0)): \n",
    "            print(\"For participants having\", df.names[i],\", the odds of having moderate or very high Anxiety/Depression is\", round(100 - np.exp(df.coeff[i])*100,2) \\\n",
    "                          , \"% lower compared to the reference group, keeping other predictors constant\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimate_ordcoef(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul><li>The intercept ‘NONE|MODERATE’ corresponds to logit[P(Y ≤ 1)]. It can be interpreted as the log of odds of having no Anxiety/depression versus having 'moderate' or ‘very high’ Anxiety/depression</li>\n",
    "    <li>Similarly, the intercept ‘MODERATE|VERY HIGH’ corresponds to logit[P(Y ≤ 2)]. It can be interpreted as the log of odds of having no or moderate Anxiety/depression versus having ‘very high’ Anxiety/depression</li>\n",
    "    <li>p-values are not significant for AGE_GROUP (40 - 54) and Race/Ethnicity=Black</li></ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong></strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_q2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df1_q2.loc[:,df1_q2.columns != 'ANXIETY_DEPRESSION'] \n",
    "y = df1_q2.loc[:,df1_q2.columns == 'ANXIETY_DEPRESSION'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cats = X.select_dtypes(include=['object','category']).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in X_cats.columns:\n",
    "    cat_list = pd.get_dummies(X[var], prefix=var, drop_first=True)\n",
    "    X.drop(var,axis=1,inplace=True)\n",
    "    X=X.join(cat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide the data into training, validation and test sets in the ration 60:20:20\n",
    "from sklearn.model_selection import train_test_split \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4,random_state=7,stratify=y,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(utils.multiclass.type_of_target(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".values will give the values in an array. (shape: (n,1) and .ravel will convert that array shape to (n, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.values.ravel()\n",
    "y_test  = y_test.values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact\n",
    "def plot_tree_rf(crit=['gini','entropy'],\n",
    "                 bootstrap=['True','False'],\n",
    "                 depth=IntSlider(min=1,max=30,value=3, continuous_update=False),\n",
    "                 forests=IntSlider(min=1,max=200,value=100,continuous_update=False),\n",
    "                 min_split=IntSlider(min=2,max=5,value=2, continuous_update=False),\n",
    "                 min_leaf=IntSlider(min=1,max=5,value=1, continuous_update=False)):\n",
    "    estimator=RandomForestClassifier(random_state=1,\n",
    "                                    criterion=crit,\n",
    "                                    bootstrap=bootstrap,\n",
    "                                    n_estimators=forests,\n",
    "                                    max_depth=depth,\n",
    "                                    min_samples_split=min_split,\n",
    "                                    min_samples_leaf=min_leaf,\n",
    "                                    n_jobs=-1,\n",
    "                                    verbose=False).fit(X_train,y_train)\n",
    "    \n",
    "    print('Random forests training accuracy: {:.3f}'.format(accuracy_score(y_train,estimator.predict(X_train))))\n",
    "    print('Random forests test accuracy: {:.3f}'.format(accuracy_score(y_test,estimator.predict(X_test))))\n",
    "    num_tree = estimator.estimators_[0]\n",
    "    print('visualizing tree:', 0)\n",
    "    graph = Source(tree.export_graphviz(num_tree,\n",
    "                                        out_file=None,\n",
    "                                        feature_names=X.columns,\n",
    "                                        class_names=['0-none','1-moderate','2-severe'],\n",
    "                                        filled=True))\n",
    "    display(Image(data=graph.pipe(format='png')))\n",
    "    return estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (25,25)\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "\n",
    "rf = RandomForestClassifier(bootstrap='True', class_weight=None, criterion='gini',\n",
    "            max_depth=6, max_features='auto', max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
    "            oob_score=False, random_state=1, verbose=False,\n",
    "            warm_start=False)\n",
    "rf.fit(X_train,y_train)\n",
    "\n",
    "plt.suptitle(\"Top 10 features that impact Anxiety/Depression\",fontsize=40,y=0.9)\n",
    "plt.savefig('C:/Users/Lakshmi/Desktop/UW DataScience application/Data 512/Final project pictures/FeaturesImpactingAnxiety.jpg')\n",
    "\n",
    "pd.Series(rf.feature_importances_, index=X.columns).nlargest(10).sort_values().plot(kind='barh');   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Likelihood Ratio Test: Testing the Joint Significance of All Predictors.</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    res_full = model\n",
    "    res_red = mass.polr('as.factor(ANXIETY_DEPRESSION) ~ 1',R_df, Hess = True, method=\"logistic\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    D1 = base.summary(res_full).rx2('deviance')[0]\n",
    "    D0 = base.summary(res_red).rx2('deviance')[0]\n",
    "    LR = D0 - D1  \n",
    "    print(\"Likelihood ratio: \",LR)\n",
    "    df=base.summary(res_full).rx2('edf')[0] - base.summary(res_red).rx2('edf')[0]\n",
    "    print(\"Effective df: \",df)\n",
    "    p_value = stats.chisqprob(LR, df)\n",
    "    print(\"Overall Likelihood Ratio Test p-value: \",p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>We can infer from these results that: </h4>\n",
    "<ul><li>Overall LRT p value is significant, we prefer full model over null model</li>\n",
    "    <li>The infliential predictors that cause an increase in Anxiety/Depression are Income loss, food insufficiency, Health status(fair and poor), AGE_GROUP25 - 39, Race Ethnicity(Hispanic, Other races, White)</li></ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>How does employment loss, income loss, food insufficiency, education interruptions, inability to meet housing expenses in Washington differ as compared to national average? </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_q3 = df1[['STATE','EMPLOSSCOVID','FOOD_INSUFF','RENT_DEBT','EDUC_INTERUPT','INCOMELOSS']].copy()\n",
    "df1_q3_pivot = pd.melt(df1_q3,id_vars=['STATE'], var_name=['INDICATOR'], value_name='VALUE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_q3_state = pd.crosstab(index=[df1_q3_pivot['STATE'],df1_q3_pivot['INDICATOR']],\\\n",
    "                             columns=df1_q3_pivot['VALUE'],normalize='index',margins=True)\n",
    "df1_q3_state.columns.rename(None, inplace=True)\n",
    "df1_q3_state.reset_index(inplace=True)\n",
    "df1_q3_state = df1_q3_state[df1_q3_state.STATE != 'All']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_q3_state = pd.melt(df1_q3_state,id_vars=['STATE','INDICATOR'], var_name='Indicator Value', value_name='VALUE')\n",
    "df1_q3_state= df1_q3_state[~df1_q3_state.INDICATOR.isin([''])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_q3_state = df1_q3_state[df1_q3_state['Indicator Value'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_q3_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30, 30))\n",
    "sns.set_context(\"notebook\", font_scale=3, rc={\"lines.linewidth\":2})\n",
    "\n",
    "graph = sns.barplot(x=\"VALUE\",\n",
    "              y=\"INDICATOR\",\n",
    "              hue = \"STATE\",\n",
    "              data=df1_q3_state,\n",
    "              palette=\"pastel\")\n",
    "              #order=df1_q1_pivot.sort_values('VALUE').INDICATOR)\n",
    "\n",
    "for i in graph.patches:\n",
    "    # get_width pulls left or right; get_y pushes up or down\n",
    "    graph.text(i.get_width()-.045, i.get_y()+.22, \\\n",
    "            str(round((i.get_width())*100, 2))+'%', fontsize=45, color='black') \n",
    "\n",
    "\n",
    "graph.set(xlabel='Percentage', title='Employment loss, income loss, education interruptions, food insufficiency, rent debt for WA versus Other States')\n",
    "\n",
    "\n",
    "L  = plt.legend(bbox_to_anchor=(1.05, 1), loc=1, borderaxespad=0.)\n",
    "graph.savefig('C:/Users/Lakshmi/Desktop/UW DataScience application/Data 512/Final project pictures/CovidImpactByState.jpg')\n",
    "plt.show(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars=['INCOMELOSS','FOOD_INSUFF','RENT_DEBT','EMPLOSSCOVID','EDUC_INTERUPT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for var in vars:\n",
    "    table = pd.crosstab(df1_q3.STATE,df1_q3[var])\n",
    "    stat, p, dof, expected = chi2_contingency(table)\n",
    "    \n",
    "    prob = 0.95\n",
    "    critical = chi2.ppf(prob, dof)\n",
    "    #print('probability=%.3f, critical=%.3f, stat=%.3f, dof=%.3f' % (prob, critical, stat, dof))\n",
    "    #dof = (rows - 1) * (cols - 1)\n",
    "    if abs(stat) >= critical:\n",
    "        print(var + ' is associated with State: Dependent (reject H0)')\n",
    "    else:\n",
    "        print(var + ' is not associated with State: Independent (fail to reject H0)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in vars:\n",
    "    y, X = dmatrices(var + \" ~ C(STATE, Treatment(reference='WASHINGTON'))\", df1_q3, return_type = 'dataframe')\n",
    "    X_std = scaler.fit_transform(X)\n",
    "    model = LogisticRegression(C = 1e9)\n",
    "    res = model.fit(X_std, y.values.ravel())\n",
    "    y_pred = model.predict_proba(X_std)[:, 1]\n",
    "    \n",
    "    #model.coef_\n",
    "    print()\n",
    "    print(\"**************Response variable = \",var, \"************** : \")\n",
    "    df = pd.concat([pd.DataFrame(X.columns),pd.DataFrame(np.transpose(model.coef_))], axis = 1)\n",
    "    df.drop(df.index[[0]],axis=0,inplace=True)\n",
    "    df.columns = pd.io.parsers.ParserBase({'names':df.columns})._maybe_dedup_names(df.columns) \n",
    "    df.rename(columns = {df.columns[0]:'predictors',df.columns[1]:'coef'}, inplace = True)\n",
    "    print(df)\n",
    "    print(\"******************************************************* : \")\n",
    "    print(\"Pseudo R^2 scores: \",r2_score(y, y_pred))\n",
    "    print()\n",
    "    estimates_interpret(df,var,\"binom\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deviance Test for logisitic regression\n",
    "#Does the model that includes the variable(s) in question tell us more about \n",
    "#the outcome (or response) variable than a model that does not include the variable(s)?\n",
    "stats.chisqprob = lambda chisq, df: stats.chi2.sf(chisq, df)\n",
    "    \n",
    "for var in vars:\n",
    "    print()\n",
    "    print(\"**************Response variable = \",var, \"************** : \")\n",
    "    data = df1_q3[[var,'STATE']].copy()\n",
    "    glm_binom_full = smf.glm(var + ' ~ 1 + C(STATE)', data=data, family=sm.families.Binomial())\n",
    "    glm_binom_red = smf.glm(var + ' ~ 1', data=data, family=sm.families.Binomial())\n",
    "    res_full = glm_binom_full.fit()\n",
    "    res_red = glm_binom_red.fit()\n",
    "    D1 = res_full.deviance\n",
    "    D0 = res_red.deviance\n",
    "    LR = D0 - D1\n",
    "    print(\"Likelihood ratio: \",LR)\n",
    "    print(\"Pseudo R^2: \",1 - (res_full.llf/res_red.llf))\n",
    "    print(\"Log-Likelihood-full: \",res_full.llf)\n",
    "    print(\"Log-Likelihood-red: \",res_red.llf)\n",
    "    df=res_full.df_model - res_red.df_model\n",
    "    p_value = stats.chisqprob(LR, df)\n",
    "    print(\"Overall Likelihood Ratio Test p-value: \",p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2 sample Z test </h3>\n",
    "<ul><li>Sample size greater than 30</li>\n",
    "<li>Independent data points</li>\n",
    "<li>Normally distributed data</li>\n",
    "<li>Randomly selected data</li>\n",
    "<li>Equal sample sizes</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from statsmodels.stats import weightstats as stests\n",
    "import random\n",
    "\n",
    "random_state = 35\n",
    "\n",
    "\n",
    "for var in vars:\n",
    "    print(\"******************\" + var + \"************************\")\n",
    "    x1 = random.sample(df1_q3.loc[df1_q3.STATE == 'OTHER',var].tolist(),k=10000)\n",
    "    x2 = random.sample(df1_q3.loc[df1_q3.STATE == 'WASHINGTON',var].tolist(),k=10000)\n",
    "    ztest ,pval = stests.ztest(x1, x2, value=0,alternative='two-sided')\n",
    "    print(float(pval))\n",
    "    if pval<0.05:\n",
    "        print(\"reject null hypothesis\")\n",
    "        print()\n",
    "    else:\n",
    "        print(\"fail to reject null hypothesis\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>PCA and K-means clustering</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state=42\n",
    "df1_q4 = df1.copy()\n",
    "df1_cats = df1_q4.select_dtypes(include=['category','object']).copy()\n",
    "\n",
    "for var in df1_cats.columns:\n",
    "    cat_list = pd.get_dummies(df1_q4[var], prefix=var, drop_first=True)\n",
    "    df1_q4.drop(var,axis=1,inplace=True)\n",
    "    df1_q4=df1_q4.join(cat_list.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_q4.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_q4.drop('WEEK',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's scale the data first\n",
    "scaler = StandardScaler()\n",
    "df1_scaled = scaler.fit_transform(df1_q4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=PCA()\n",
    "model.fit(df1_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model.explained_variance_ratio_.cumsum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the explained variances\n",
    "plt.figure(figsize=(30,10))\n",
    "plt.plot(range(1,34), model.explained_variance_ratio_.cumsum(), marker='o',linestyle='--')\n",
    "plt.title('Explained variance by components',fontsize=30)\n",
    "plt.xlabel('PCA features', fontsize = 35)\n",
    "plt.ylabel('variance %', fontsize = 35)\n",
    "\n",
    "plt.tick_params(labelsize=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state=42\n",
    "model=PCA(n_components=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca = pd.DataFrame(df1_scaled, columns=df1_q4.columns)\n",
    "\n",
    "pca_scores = model.fit_transform(df_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> K-means clustering </h3>\n",
    "\n",
    "- The elbow method is a heuristic method of interpretation and validation of consistency within cluster analysis designed to help find the appropriate number of clusters in a dataset. \n",
    "- If the line chart looks like an arm, then the \"elbow\" on the arm is the value of k that is the best.\n",
    "- Source: \n",
    "  - https://en.wikipedia.org/wiki/Elbow_method_(clustering)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state=42\n",
    "wcss=[]\n",
    "for i in range(1,12):\n",
    "    kmeans_pca=KMeans(n_clusters=i,init='k-means++',random_state=42)\n",
    "    kmeans_pca.fit(pca_scores)\n",
    "    wcss.append(kmeans_pca.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30,15))\n",
    "plt.plot(range(1,12), wcss, marker='o',linestyle='--')\n",
    "plt.title('k-means with pca',fontsize=30)\n",
    "plt.xlabel('Number of clusters',fontsize=30)\n",
    "plt.ylabel('wcss',fontsize=40)\n",
    "\n",
    "plt.tick_params(labelsize=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(3,init='k-means++',random_state=42)\n",
    "kmeans.fit(pca_scores)\n",
    "labels = kmeans.labels_ # labels associated with each data point\n",
    "# index of the cluster centroid that is closest to x(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.cluster_centers_.shape # centroid for clusters = mu_c(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca_kmean_seg = pd.concat([df1_q4.reset_index(drop=True),pd.DataFrame(pca_scores)],axis=1)\n",
    "df_pca_kmean_seg.columns.values[-2:] = ['pca1','pca2']\n",
    "df_pca_kmean_seg['seg_labels'] = labels\n",
    "df_pca_kmean_seg['Segment'] = df_pca_kmean_seg['seg_labels'].map({0:'first',1:'second',2:'third'})\n",
    "df_pca_kmean_seg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "h<ol><strong>First segment :</strong>\n",
    "    <ul><li>These participants do not have any Anxiety or depression</li>\n",
    "        <li>They have neither any interest nor worries. </li>\n",
    "        <li>They have High school diploma or GED degree, mostly belonging to the age group AGE_GROUP 25 - 54 and earning about 25,000− 74,999 dollars</li>\n",
    "    </ul>\n",
    "\n",
    "<strong>Second segment :</strong>\n",
    "    <ul><li>These participants have moderate Anxiety or depression</li>\n",
    "        <li>They have very high interest and worry levels. </li>\n",
    "        <li>They have Some college/associate's degree, mostly belonging to the age group AGE_GROUP 25 - 64 and earning below 74,999 dollars</li>\n",
    "    </ul>\n",
    "    \n",
    "    \n",
    "<strong>Third segment :</strong>\n",
    "    <ul><li>These participants have very high Anxiety or depression</li>\n",
    "        <li>They have moderate interest and worry levels. </li>\n",
    "        <li>They have either college/associate's degree or High school diploma or GED, mostly belonging to the age group AGE_GROUP 40 - above and earning 25,000− 150000 dollars</li>\n",
    "    </ul>\n",
    "\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state=42\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\":2})\n",
    "\n",
    "ax = sns.scatterplot(x=\"pca1\", y=\"pca2\", hue = \"Segment\", data = df_pca_kmean_seg, palette =['red','green','yellow'])\n",
    "#ax.figure.savefig('C:/Users/Lakshmi/Desktop/UW DataScience application/558 ML/Coursera ML/PCA/PCA.jpg');\n",
    "\n",
    "L  = plt.legend(bbox_to_anchor=(1.05, 1), loc=4, borderaxespad=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "def myplot(score,coeff,labels=None):\n",
    "    xs = score[:,0]\n",
    "    ys = score[:,1]\n",
    "    n = coeff.shape[0]\n",
    "    scalex = 1.0/(xs.max() - xs.min())\n",
    "    scaley = 1.0/(ys.max() - ys.min())\n",
    "    #plt.scatter(xs * scalex,ys * scaley,s=8,color = 'yellow')\n",
    "    ax = sns.scatterplot(xs * scalex,ys * scaley, hue = \"Segment\", data = df_pca_kmean_seg, palette =['red','green','yellow'])\n",
    "    for i in range(n):\n",
    "        plt.arrow(0, 0, coeff[i,0], coeff[i,1],color = 'blue',alpha = 0.5)\n",
    "        if labels is None:\n",
    "            plt.text(coeff[i,0]* 1.15, coeff[i,1] * 1.15, \"Var\"+str(i+1), color = 'blue', ha = 'center', va = 'center',alpha=0.9)\n",
    "        else:\n",
    "            plt.text(coeff[i,0]* 1.15, coeff[i,1] * 1.15, labels[i] , color = 'black', ha = 'center', va = 'center',alpha=0.9)\n",
    " \n",
    "    plt.xlabel(\"PC{}\".format(1))\n",
    "    plt.ylabel(\"PC{}\".format(2))\n",
    "    plt.grid()\n",
    "\n",
    "myplot(pca_scores[:,0:2],np.transpose(model.components_[0:2, :]),list(df_pca_kmean_seg.columns))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal component 1 has strong positive loadings for Very high and moderate levels of moderate or very high levels of worry and interest, Income loss, Employment loss, Food Insufficiency,\n",
    "Fair Health status, Income level between 25,000− 74,999,  college/associate's degree or High school diploma or GED, Race Ethnicity is either black or hispanic and strong negative loadings for White, Excellent healthstatus,Bachelor's degree or higher, income over 75k dollars, high education qualification, Age group 65 and above, Food insufficiency not due to covid, gender male . As these are analyses of covid impacts, this likely reflects relatively impacts to poor minority groups at positive axis 1 scores and increasing rich white at negative axis 1 scores.\n",
    "\n",
    "\n",
    "Principal component 2 has strong positive loadings for Very high levels of worry and interest, moderate levels of Anxiety and depression and strong negative loadings for high levels of Anxiety and depression and moderate levels of worry and interest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## project data into Principal Component space\n",
    "# 0,1 denote PC1 and PC2; change values for other PCs\n",
    "Segments = df_pca_kmean_seg.Segment\n",
    "\n",
    "xvector = model.components_[0]  # see 'prcomp(data)$rotation' in R\n",
    "yvector = model.components_[1]\n",
    "\n",
    "xs = pca_scores[:, 0]  # see 'prcomp(data)$x' in R\n",
    "ys = pca_scores[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_components = pd.DataFrame({'Variables':df1_q4.columns.values,'Loadings X': xvector, 'Loadings Y': yvector})\n",
    "pca_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp1= pca_components.loc[((pca_components['Loadings X'] >= -0.294) \\\n",
    "                          & (pca_components['Loadings X'] < 0.1) \\\n",
    "                          & (pca_components['Loadings Y'] > -0.15) \\\n",
    "                         & (pca_components['Loadings Y'] < 0.5)),:]\n",
    "grp2= pca_components.loc[((pca_components['Loadings X'] >= -0.1) \\\n",
    "                          & (pca_components['Loadings X'] <= 0.6) \\\n",
    "                          & (pca_components['Loadings Y'] > -0.4) \\\n",
    "                         & (pca_components['Loadings Y'] < 0.2)),:]\n",
    "grp3= pca_components.loc[((pca_components['Loadings X'] > -0.1) \n",
    "                          & (pca_components['Loadings Y'] > 0)),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
